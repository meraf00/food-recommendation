{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import preprocessing\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for tokenizer\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def recipe_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,' ').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "\n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for word embedding using Word2Vec\n",
    "def word_embedding(sampled_data, column):\n",
    "    tokenized_data = sampled_data[column].apply(recipe_tokenizer)\n",
    "\n",
    "    model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_embeddings(sampled_data):    \n",
    "    embeddings = word_embedding(sampled_data, 'Main food description')\n",
    "    \n",
    "    sampled_data['text_data'] = sampled_data[['WWEIA Category description', 'name_of_eating_occasion', 'source_of_food']].astype(str).agg(' '.join, axis=1)\n",
    "    \n",
    "    sampled_data['text_data'] = sampled_data['text_data'].str.lower()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=5,\n",
    "                                 tokenizer=recipe_tokenizer)\n",
    "    vectorized_data = vectorizer.fit_transform(sampled_data['text_data'])\n",
    "    \n",
    "    ingredient_embeddings = [np.mean([embeddings[word] for word in recipe_tokenizer(ingredients) if word in embeddings]\n",
    "                                      or [np.zeros(100)], axis=0) for ingredients in sampled_data['Main food description']]\n",
    "    \n",
    "    combined_embeddings = np.concatenate([vectorized_data.toarray(), np.array(ingredient_embeddings)], axis=1)\n",
    "        \n",
    "    with open('combined_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(combined_embeddings, f)\n",
    "        \n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.load_food_pref_dataset()[['usda_food_code', 'WWEIA Category description', 'name_of_eating_occasion', 'source_of_food', 'Main food description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['usda_food_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usda_food_code</th>\n",
       "      <th>WWEIA Category description</th>\n",
       "      <th>name_of_eating_occasion</th>\n",
       "      <th>source_of_food</th>\n",
       "      <th>Main food description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28320300</td>\n",
       "      <td>Soups</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Store - grocery/supermarket</td>\n",
       "      <td>Pork with vegetable excluding carrots, broccol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>91746110</td>\n",
       "      <td>Candy containing chocolate</td>\n",
       "      <td>Snack</td>\n",
       "      <td>Child/Adult care center</td>\n",
       "      <td>Chocolate candy, candy shell with nuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>58106210</td>\n",
       "      <td>Pizza</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Child/Adult care center</td>\n",
       "      <td>Pizza, cheese, from restaurant or fast food, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>64104010</td>\n",
       "      <td>Apple juice</td>\n",
       "      <td>Snack</td>\n",
       "      <td>Store - grocery/supermarket</td>\n",
       "      <td>Apple juice, 100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>11710801</td>\n",
       "      <td>Formula, ready-to-feed</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Store - grocery/supermarket</td>\n",
       "      <td>Toddler formula, PediaSure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     usda_food_code  WWEIA Category description name_of_eating_occasion   \n",
       "0          28320300                       Soups                  Dinner  \\\n",
       "11         91746110  Candy containing chocolate                   Snack   \n",
       "55         58106210                       Pizza                   Lunch   \n",
       "59         64104010                 Apple juice                   Snack   \n",
       "892        11710801      Formula, ready-to-feed                  Dinner   \n",
       "\n",
       "                  source_of_food   \n",
       "0    Store - grocery/supermarket  \\\n",
       "11       Child/Adult care center   \n",
       "55       Child/Adult care center   \n",
       "59   Store - grocery/supermarket   \n",
       "892  Store - grocery/supermarket   \n",
       "\n",
       "                                 Main food description  \n",
       "0    Pork with vegetable excluding carrots, broccol...  \n",
       "11              Chocolate candy, candy shell with nuts  \n",
       "55   Pizza, cheese, from restaurant or fast food, N...  \n",
       "59                                   Apple juice, 100%  \n",
       "892                         Toddler formula, PediaSure  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_vectorizer():\n",
    "    with open('combined_embeddings.pkl', 'rb') as f:\n",
    "        combined_embeddings = pickle.load(f)\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    return combined_embeddings, vectorizer\n",
    "\n",
    "\n",
    "def find_similar_recipes(sampled_data, user_input, num_similar=10):\n",
    "    try:\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "    except FileNotFoundError:\n",
    "        precompute_embeddings(sampled_data)\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "        \n",
    "    user_data = pd.DataFrame({'text_data': [user_input]})\n",
    "    user_data['text_data'] = user_data['text_data'].str.lower()\n",
    "    \n",
    "    user_vectorized_data = vectorizer.transform(user_data['text_data'])\n",
    "    \n",
    "    num_missing_features = combined_embeddings.shape[1] - user_vectorized_data.shape[1]\n",
    "    if num_missing_features > 0:        \n",
    "        user_vectorized_data = np.pad(user_vectorized_data.toarray(), ((0, 0), (0, num_missing_features)))\n",
    "    \n",
    "    cosine_sim_matrix = cosine_similarity(user_vectorized_data, combined_embeddings)\n",
    "    \n",
    "    similar_recipes = cosine_sim_matrix[0].argsort()[::-1]\n",
    "    \n",
    "    similar_recipe_names = sampled_data.iloc[similar_recipes]['Main food description'].tolist()\n",
    "\n",
    "    return similar_recipe_names[:num_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\L\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Oatmeal, multigrain',\n",
       " 'Oatmeal, instant, maple flavored, fat added',\n",
       " 'Oatmeal, instant, maple flavored, no added fat',\n",
       " 'Oatmeal, NFS',\n",
       " 'Oatmeal, regular or quick, made with non-dairy milk, no added fat',\n",
       " 'Oatmeal, regular or quick, made with non-dairy milk, fat added',\n",
       " 'Oatmeal, regular or quick, made with milk, fat added',\n",
       " 'Oatmeal, regular or quick, made with water, fat added',\n",
       " 'Oatmeal, regular or quick, made with water, no added fat',\n",
       " 'Oatmeal, instant, plain, made with non-dairy milk, no added fat']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_recipes(df, \"Oatmeal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
